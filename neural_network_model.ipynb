{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f307d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import operator\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "# classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# nn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,  BatchNormalization, Flatten, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# never forget to supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca085cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movies_final.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d90083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'averageRating'\n",
    "\n",
    "conditions = [\n",
    "    df[col].between(0,2),\n",
    "    df[col].between(2,4),\n",
    "    df[col].between(4,6),\n",
    "    df[col].between(6,8),\n",
    "    df[col].between(8,10)\n",
    "]\n",
    "\n",
    "ranks = ['rank1', 'rank2', 'rank3','rank4', 'rank5']\n",
    "df['ratingClass'] = np.select(conditions, ranks, default=np.nan)\n",
    "df['ratingClass'] = df['ratingClass'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f92e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['averageRating', 'ratingClass', 'tconst', 'primaryTitle'])\n",
    "y = df[['ratingClass']]\n",
    "y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "398d1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a010a8c",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "032dd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#                      ('scaler',StandardScaler()),\n",
    "#                      ('model',Lasso())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc3d4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = GridSearchCV(pipeline,\n",
    "#                       {'model__alpha':np.arange(0.1,10,0.1)},\n",
    "#                       cv = 5, scoring=\"neg_mean_squared_error\",verbose=3\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f79d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients = search.best_estimator_.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = np.abs(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd07a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(features)[importance > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7c002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.array(features)[importance == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be968bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(features)[importance != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df074728",
   "metadata": {},
   "source": [
    "Chi-sq test for feature selection: The results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = SelectKBest(score_func=chi2, k='all')\n",
    "# fs.fit(X_train, y_train)\n",
    "# X_train_fs = fs.transform(X_train)\n",
    "# X_test_fs = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(fs.scores_)):\n",
    "#     print('feature %d: %f' % (i, fs.scores_[i]))\n",
    "    \n",
    "# pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660936b8",
   "metadata": {},
   "source": [
    "# Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['averageRating', 'ratingClass', 'tconst', 'primaryTitle'])\n",
    "y = df[['ratingClass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "features = X.columns\n",
    "\n",
    "for i in range(len(fs.scores_)):\n",
    "    scores[features[i]] = fs.scores_[i]\n",
    "#     if i not in scores:\n",
    "#         scores.append('Feature %d: %f' % (i, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ec171",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc550d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = [feature for feature in sorted_scores if feature[1] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "\n",
    "for i in filtered_features:\n",
    "    selected_features.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = ['ratingClass', 'averageRating']\n",
    "\n",
    "selected_features.extend(y_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb588b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns=['averageRating', 'ratingClass'])\n",
    "y = df2[['ratingClass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc28a9c",
   "metadata": {},
   "source": [
    "# Oversampling\n",
    "With SMOTE, the minority class is over-sampled by creating “synthetic” examples\n",
    "These introduced synthetic examples are based along the line segments joining a defined number of k minority class nearest neighbours, which is in the imblearn package is set at five by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before oversampling: \", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebff882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83eb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee887133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After oversampling: \", y_train_SMOTE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = MLPClassifier(hidden_layer_sizes=(150,100,50)\n",
    "#                            , max_iter=300\n",
    "#                            ,activation = 'relu'\n",
    "#                            ,solver='adam',random_state=42)\n",
    "\n",
    "# classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91f99b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33b0afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ac68417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.5428467683369644\n"
     ]
    }
   ],
   "source": [
    "# print(\"Accuracy of MLPClassifier : \", accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e24f2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5721\n"
     ]
    }
   ],
   "source": [
    "# mlp = MLPClassifier(solver=\"adam\", activation=\"logistic\", alpha=1e-5, hidden_layer_sizes=(100, 20),\n",
    "#                     random_state=42)\n",
    "\n",
    "# print(\"Score: {:.4f}\".format(cross_val_score(mlp, X_train, y_train, cv=3).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58a61ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.fit(X_train, y_train)\n",
    "# y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3c40bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f32458da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier :  0.5750907770515614\n"
     ]
    }
   ],
   "source": [
    "# print(\"Accuracy of MLPClassifier : \", accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=SVC()\n",
    "# clf_SMOTE = model.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "# pred_SMOTE = clf_SMOTE.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "55e44ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(input_dim=(len(X_train.columns)), classes = 5):\n",
    "    \n",
    "    model = Sequential([\n",
    "        \n",
    "\n",
    "    Dense(350, input_dim=(len(X_train.columns)), activation = 'relu', kernel_initializer='normal', kernel_regularizer=l2(0.0005), name = 'layer_1'),\n",
    "    \n",
    "\n",
    "    Dense(100, input_dim=(len(X_train.columns)), kernel_initializer='normal', name = 'layer_2'),\n",
    "    \n",
    "\n",
    "    BatchNormalization(name = 'batchnorm_1'),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.25, name = 'dropout_1'),\n",
    "    Dense(64, input_dim=(len(X_train.columns)), kernel_initializer='normal', activation = 'relu', kernel_regularizer=l2(0.0005), name = 'layer_3'),\n",
    "        \n",
    "\n",
    "    Dense(64, input_dim=(len(X_train.columns)), kernel_initializer='normal', name = 'layer_4', use_bias=False),\n",
    "        \n",
    "\n",
    "    BatchNormalization(name = 'batchnorm_2'),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.25, name = 'dropout_2'),\n",
    "    Flatten(name = 'flatten'),\n",
    "\n",
    "        \n",
    "\n",
    "    Dense(256, input_dim=(len(X_train.columns)), kernel_initializer='normal', name = 'fully_connected_1', use_bias=False),\n",
    "        \n",
    "\n",
    "    BatchNormalization(name = 'batchnorm_3'),\n",
    "    Activation(\"relu\"),\n",
    "        \n",
    "\n",
    "    Dense(128, input_dim=(len(X_train.columns)), kernel_initializer='normal', name = 'fully_connected_2', use_bias=False),\n",
    "        \n",
    "\n",
    "    BatchNormalization(name = 'batchnorm_4'),\n",
    "    Activation(\"relu\"),\n",
    "        \n",
    "\n",
    "    Dense(84, input_dim=(len(X_train.columns)), name = 'fully_connected_3', use_bias=False),\n",
    "        \n",
    "\n",
    "    BatchNormalization(name = 'batchnorm_5'),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.25, name = 'dropout_3'),\n",
    "\n",
    "\n",
    "    Dense(units = 5, activation = 'softmax', name = 'output')  \n",
    "    ])\n",
    "    \n",
    "    model._name = 'final_model'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a88353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_predictor = final_model(input_dim=(len(X_train.columns)), classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26f104ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_predictor.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f11b2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 2)\n",
    "mc = ModelCheckpoint(\"/home/creeg/projects/audible/notebooks/rating_predictor.h5\", monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "cb = [mc, variable_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d11b3318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 1.0798 - accuracy: 0.5303\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55407, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 1.0797 - accuracy: 0.5304 - val_loss: 1.0201 - val_accuracy: 0.5541 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.5463\n",
      "Epoch 2: val_accuracy did not improve from 0.55407\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 1.0339 - accuracy: 0.5463 - val_loss: 1.0118 - val_accuracy: 0.5496 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 1.0236 - accuracy: 0.5502\n",
      "Epoch 3: val_accuracy improved from 0.55407 to 0.55592, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 1.0232 - accuracy: 0.5503 - val_loss: 1.0078 - val_accuracy: 0.5559 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 1.0158 - accuracy: 0.5551\n",
      "Epoch 4: val_accuracy improved from 0.55592 to 0.55846, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 1.0158 - accuracy: 0.5551 - val_loss: 0.9924 - val_accuracy: 0.5585 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 1.0096 - accuracy: 0.5541\n",
      "Epoch 5: val_accuracy improved from 0.55846 to 0.56052, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 1.0095 - accuracy: 0.5542 - val_loss: 0.9858 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 1.0072 - accuracy: 0.5557\n",
      "Epoch 6: val_accuracy did not improve from 0.56052\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 1.0073 - accuracy: 0.5557 - val_loss: 0.9846 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 1.0047 - accuracy: 0.5557\n",
      "Epoch 7: val_accuracy improved from 0.56052 to 0.56124, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 1.0046 - accuracy: 0.5557 - val_loss: 0.9854 - val_accuracy: 0.5612 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 1.0032 - accuracy: 0.5550\n",
      "Epoch 8: val_accuracy improved from 0.56124 to 0.56170, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 1.0032 - accuracy: 0.5550 - val_loss: 0.9828 - val_accuracy: 0.5617 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 1.0008 - accuracy: 0.5546\n",
      "Epoch 9: val_accuracy improved from 0.56170 to 0.56242, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 1.0009 - accuracy: 0.5546 - val_loss: 0.9822 - val_accuracy: 0.5624 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9986 - accuracy: 0.5578\n",
      "Epoch 10: val_accuracy improved from 0.56242 to 0.56267, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9985 - accuracy: 0.5578 - val_loss: 0.9819 - val_accuracy: 0.5627 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9993 - accuracy: 0.5574\n",
      "Epoch 11: val_accuracy improved from 0.56267 to 0.56609, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9992 - accuracy: 0.5574 - val_loss: 0.9783 - val_accuracy: 0.5661 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9974 - accuracy: 0.5586\n",
      "Epoch 12: val_accuracy did not improve from 0.56609\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9975 - accuracy: 0.5586 - val_loss: 0.9826 - val_accuracy: 0.5643 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9960 - accuracy: 0.5578\n",
      "Epoch 13: val_accuracy improved from 0.56609 to 0.56618, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9962 - accuracy: 0.5578 - val_loss: 0.9766 - val_accuracy: 0.5662 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9937 - accuracy: 0.5583\n",
      "Epoch 14: val_accuracy did not improve from 0.56618\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 0.9937 - accuracy: 0.5583 - val_loss: 0.9853 - val_accuracy: 0.5613 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9936 - accuracy: 0.5599\n",
      "Epoch 15: val_accuracy improved from 0.56618 to 0.56754, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9936 - accuracy: 0.5599 - val_loss: 0.9757 - val_accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9943 - accuracy: 0.5592\n",
      "Epoch 16: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9944 - accuracy: 0.5592 - val_loss: 0.9761 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9927 - accuracy: 0.5601\n",
      "Epoch 17: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9929 - accuracy: 0.5602 - val_loss: 0.9767 - val_accuracy: 0.5661 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9936 - accuracy: 0.5610\n",
      "Epoch 18: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9933 - accuracy: 0.5612 - val_loss: 0.9794 - val_accuracy: 0.5627 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9923 - accuracy: 0.5606\n",
      "Epoch 19: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9923 - accuracy: 0.5606 - val_loss: 0.9801 - val_accuracy: 0.5635 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9933 - accuracy: 0.5609\n",
      "Epoch 20: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9932 - accuracy: 0.5609 - val_loss: 0.9766 - val_accuracy: 0.5652 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9911 - accuracy: 0.5605\n",
      "Epoch 21: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9911 - accuracy: 0.5605 - val_loss: 0.9732 - val_accuracy: 0.5668 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9919 - accuracy: 0.5597\n",
      "Epoch 22: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9919 - accuracy: 0.5597 - val_loss: 0.9775 - val_accuracy: 0.5620 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9910 - accuracy: 0.5616\n",
      "Epoch 23: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9912 - accuracy: 0.5616 - val_loss: 0.9763 - val_accuracy: 0.5655 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9894 - accuracy: 0.5614\n",
      "Epoch 24: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9896 - accuracy: 0.5613 - val_loss: 0.9751 - val_accuracy: 0.5669 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9894 - accuracy: 0.5613\n",
      "Epoch 25: val_accuracy did not improve from 0.56754\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9892 - accuracy: 0.5614 - val_loss: 0.9770 - val_accuracy: 0.5657 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9893 - accuracy: 0.5618\n",
      "Epoch 26: val_accuracy improved from 0.56754 to 0.56832, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9893 - accuracy: 0.5618 - val_loss: 0.9727 - val_accuracy: 0.5683 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 0.9894 - accuracy: 0.5623\n",
      "Epoch 27: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 0.9895 - accuracy: 0.5622 - val_loss: 0.9799 - val_accuracy: 0.5672 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9881 - accuracy: 0.5605\n",
      "Epoch 28: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9881 - accuracy: 0.5606 - val_loss: 0.9737 - val_accuracy: 0.5666 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9889 - accuracy: 0.5602\n",
      "Epoch 29: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9889 - accuracy: 0.5602 - val_loss: 0.9757 - val_accuracy: 0.5663 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9874 - accuracy: 0.5624\n",
      "Epoch 30: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9873 - accuracy: 0.5624 - val_loss: 0.9736 - val_accuracy: 0.5680 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9875 - accuracy: 0.5620\n",
      "Epoch 31: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9875 - accuracy: 0.5620 - val_loss: 0.9768 - val_accuracy: 0.5663 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9891 - accuracy: 0.5615\n",
      "Epoch 32: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9890 - accuracy: 0.5615 - val_loss: 0.9745 - val_accuracy: 0.5681 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9887 - accuracy: 0.5612\n",
      "Epoch 33: val_accuracy did not improve from 0.56832\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9886 - accuracy: 0.5612 - val_loss: 0.9724 - val_accuracy: 0.5664 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9872 - accuracy: 0.5628\n",
      "Epoch 34: val_accuracy improved from 0.56832 to 0.56875, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9872 - accuracy: 0.5628 - val_loss: 0.9717 - val_accuracy: 0.5687 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9874 - accuracy: 0.5620\n",
      "Epoch 35: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9875 - accuracy: 0.5619 - val_loss: 0.9761 - val_accuracy: 0.5668 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9872 - accuracy: 0.5628\n",
      "Epoch 36: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9872 - accuracy: 0.5628 - val_loss: 0.9796 - val_accuracy: 0.5624 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9884 - accuracy: 0.5629\n",
      "Epoch 37: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9884 - accuracy: 0.5631 - val_loss: 0.9756 - val_accuracy: 0.5643 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9878 - accuracy: 0.5635\n",
      "Epoch 38: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9879 - accuracy: 0.5635 - val_loss: 0.9741 - val_accuracy: 0.5676 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9862 - accuracy: 0.5632\n",
      "Epoch 39: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9864 - accuracy: 0.5631 - val_loss: 0.9723 - val_accuracy: 0.5677 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 0.9877 - accuracy: 0.5634\n",
      "Epoch 40: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9877 - accuracy: 0.5634 - val_loss: 0.9791 - val_accuracy: 0.5613 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9865 - accuracy: 0.5628\n",
      "Epoch 41: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9865 - accuracy: 0.5628 - val_loss: 0.9713 - val_accuracy: 0.5682 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9868 - accuracy: 0.5629\n",
      "Epoch 42: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9868 - accuracy: 0.5629 - val_loss: 0.9719 - val_accuracy: 0.5659 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9867 - accuracy: 0.5623\n",
      "Epoch 43: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9867 - accuracy: 0.5623 - val_loss: 0.9764 - val_accuracy: 0.5682 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 0.9878 - accuracy: 0.5626\n",
      "Epoch 44: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9878 - accuracy: 0.5626 - val_loss: 0.9763 - val_accuracy: 0.5682 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9853 - accuracy: 0.5643\n",
      "Epoch 45: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9854 - accuracy: 0.5642 - val_loss: 0.9718 - val_accuracy: 0.5668 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9858 - accuracy: 0.5632\n",
      "Epoch 46: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9860 - accuracy: 0.5630 - val_loss: 0.9745 - val_accuracy: 0.5655 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9862 - accuracy: 0.5626\n",
      "Epoch 47: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9862 - accuracy: 0.5626 - val_loss: 0.9750 - val_accuracy: 0.5642 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9868 - accuracy: 0.5624\n",
      "Epoch 48: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9864 - accuracy: 0.5624 - val_loss: 0.9748 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9870 - accuracy: 0.5626\n",
      "Epoch 49: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9871 - accuracy: 0.5626 - val_loss: 0.9742 - val_accuracy: 0.5674 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "3839/3856 [============================>.] - ETA: 0s - loss: 0.9866 - accuracy: 0.5633\n",
      "Epoch 50: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9866 - accuracy: 0.5633 - val_loss: 0.9729 - val_accuracy: 0.5658 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9867 - accuracy: 0.5625\n",
      "Epoch 51: val_accuracy did not improve from 0.56875\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9866 - accuracy: 0.5627 - val_loss: 0.9715 - val_accuracy: 0.5661 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9765 - accuracy: 0.5653\n",
      "Epoch 52: val_accuracy improved from 0.56875 to 0.56990, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9762 - accuracy: 0.5655 - val_loss: 0.9664 - val_accuracy: 0.5699 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9743 - accuracy: 0.5687\n",
      "Epoch 53: val_accuracy did not improve from 0.56990\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9742 - accuracy: 0.5687 - val_loss: 0.9650 - val_accuracy: 0.5686 - lr: 2.0000e-04\n",
      "Epoch 54/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.5683\n",
      "Epoch 54: val_accuracy improved from 0.56990 to 0.57047, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9722 - accuracy: 0.5683 - val_loss: 0.9629 - val_accuracy: 0.5705 - lr: 2.0000e-04\n",
      "Epoch 55/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9723 - accuracy: 0.5680\n",
      "Epoch 55: val_accuracy improved from 0.57047 to 0.57072, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9723 - accuracy: 0.5680 - val_loss: 0.9631 - val_accuracy: 0.5707 - lr: 2.0000e-04\n",
      "Epoch 56/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9714 - accuracy: 0.5695\n",
      "Epoch 56: val_accuracy improved from 0.57072 to 0.57120, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9715 - accuracy: 0.5695 - val_loss: 0.9620 - val_accuracy: 0.5712 - lr: 2.0000e-04\n",
      "Epoch 57/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.5704\n",
      "Epoch 57: val_accuracy did not improve from 0.57120\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 0.9715 - accuracy: 0.5704 - val_loss: 0.9619 - val_accuracy: 0.5708 - lr: 2.0000e-04\n",
      "Epoch 58/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9709 - accuracy: 0.5690\n",
      "Epoch 58: val_accuracy did not improve from 0.57120\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9708 - accuracy: 0.5691 - val_loss: 0.9616 - val_accuracy: 0.5707 - lr: 2.0000e-04\n",
      "Epoch 59/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9704 - accuracy: 0.5688\n",
      "Epoch 59: val_accuracy did not improve from 0.57120\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9703 - accuracy: 0.5688 - val_loss: 0.9615 - val_accuracy: 0.5692 - lr: 2.0000e-04\n",
      "Epoch 60/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9694 - accuracy: 0.5694\n",
      "Epoch 60: val_accuracy improved from 0.57120 to 0.57132, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9694 - accuracy: 0.5694 - val_loss: 0.9614 - val_accuracy: 0.5713 - lr: 2.0000e-04\n",
      "Epoch 61/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9696 - accuracy: 0.5697\n",
      "Epoch 61: val_accuracy did not improve from 0.57132\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9696 - accuracy: 0.5698 - val_loss: 0.9608 - val_accuracy: 0.5706 - lr: 2.0000e-04\n",
      "Epoch 62/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9687 - accuracy: 0.5703\n",
      "Epoch 62: val_accuracy did not improve from 0.57132\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9684 - accuracy: 0.5703 - val_loss: 0.9607 - val_accuracy: 0.5705 - lr: 2.0000e-04\n",
      "Epoch 63/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9681 - accuracy: 0.5696\n",
      "Epoch 63: val_accuracy improved from 0.57132 to 0.57138, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9683 - accuracy: 0.5695 - val_loss: 0.9611 - val_accuracy: 0.5714 - lr: 2.0000e-04\n",
      "Epoch 64/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9683 - accuracy: 0.5699\n",
      "Epoch 64: val_accuracy improved from 0.57138 to 0.57217, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9682 - accuracy: 0.5699 - val_loss: 0.9615 - val_accuracy: 0.5722 - lr: 2.0000e-04\n",
      "Epoch 65/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9675 - accuracy: 0.5716\n",
      "Epoch 65: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9677 - accuracy: 0.5715 - val_loss: 0.9608 - val_accuracy: 0.5712 - lr: 2.0000e-04\n",
      "Epoch 66/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9681 - accuracy: 0.5703\n",
      "Epoch 66: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9678 - accuracy: 0.5705 - val_loss: 0.9614 - val_accuracy: 0.5703 - lr: 2.0000e-04\n",
      "Epoch 67/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9670 - accuracy: 0.5695\n",
      "Epoch 67: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9672 - accuracy: 0.5694 - val_loss: 0.9602 - val_accuracy: 0.5704 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9680 - accuracy: 0.5691\n",
      "Epoch 68: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9680 - accuracy: 0.5691 - val_loss: 0.9606 - val_accuracy: 0.5693 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9677 - accuracy: 0.5702\n",
      "Epoch 69: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9677 - accuracy: 0.5702 - val_loss: 0.9611 - val_accuracy: 0.5705 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9674 - accuracy: 0.5696\n",
      "Epoch 70: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9674 - accuracy: 0.5696 - val_loss: 0.9605 - val_accuracy: 0.5705 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9664 - accuracy: 0.5698\n",
      "Epoch 71: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9665 - accuracy: 0.5699 - val_loss: 0.9595 - val_accuracy: 0.5700 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9664 - accuracy: 0.5701\n",
      "Epoch 72: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9665 - accuracy: 0.5701 - val_loss: 0.9614 - val_accuracy: 0.5695 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9673 - accuracy: 0.5701\n",
      "Epoch 73: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9675 - accuracy: 0.5699 - val_loss: 0.9600 - val_accuracy: 0.5721 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9661 - accuracy: 0.5707\n",
      "Epoch 74: val_accuracy did not improve from 0.57217\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9662 - accuracy: 0.5706 - val_loss: 0.9612 - val_accuracy: 0.5706 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9657 - accuracy: 0.5720\n",
      "Epoch 75: val_accuracy improved from 0.57217 to 0.57223, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9655 - accuracy: 0.5721 - val_loss: 0.9601 - val_accuracy: 0.5722 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9659 - accuracy: 0.5720\n",
      "Epoch 76: val_accuracy did not improve from 0.57223\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9659 - accuracy: 0.5720 - val_loss: 0.9602 - val_accuracy: 0.5707 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9650 - accuracy: 0.5717\n",
      "Epoch 77: val_accuracy improved from 0.57223 to 0.57244, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9650 - accuracy: 0.5716 - val_loss: 0.9601 - val_accuracy: 0.5724 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9655 - accuracy: 0.5710\n",
      "Epoch 78: val_accuracy did not improve from 0.57244\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9654 - accuracy: 0.5710 - val_loss: 0.9598 - val_accuracy: 0.5723 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9661 - accuracy: 0.5696\n",
      "Epoch 79: val_accuracy did not improve from 0.57244\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 0.9660 - accuracy: 0.5697 - val_loss: 0.9605 - val_accuracy: 0.5723 - lr: 2.0000e-04\n",
      "Epoch 80/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9647 - accuracy: 0.5709\n",
      "Epoch 80: val_accuracy did not improve from 0.57244\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9651 - accuracy: 0.5708 - val_loss: 0.9616 - val_accuracy: 0.5697 - lr: 2.0000e-04\n",
      "Epoch 81/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9652 - accuracy: 0.5709\n",
      "Epoch 81: val_accuracy improved from 0.57244 to 0.57256, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9652 - accuracy: 0.5710 - val_loss: 0.9598 - val_accuracy: 0.5726 - lr: 2.0000e-04\n",
      "Epoch 82/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9619 - accuracy: 0.5724\n",
      "Epoch 82: val_accuracy improved from 0.57256 to 0.57344, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9619 - accuracy: 0.5724 - val_loss: 0.9569 - val_accuracy: 0.5734 - lr: 4.0000e-05\n",
      "Epoch 83/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9606 - accuracy: 0.5730\n",
      "Epoch 83: val_accuracy did not improve from 0.57344\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9606 - accuracy: 0.5730 - val_loss: 0.9577 - val_accuracy: 0.5714 - lr: 4.0000e-05\n",
      "Epoch 84/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9618 - accuracy: 0.5724\n",
      "Epoch 84: val_accuracy did not improve from 0.57344\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9619 - accuracy: 0.5723 - val_loss: 0.9570 - val_accuracy: 0.5734 - lr: 4.0000e-05\n",
      "Epoch 85/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.5725\n",
      "Epoch 85: val_accuracy did not improve from 0.57344\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9605 - accuracy: 0.5725 - val_loss: 0.9568 - val_accuracy: 0.5726 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9593 - accuracy: 0.5746\n",
      "Epoch 86: val_accuracy did not improve from 0.57344\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9594 - accuracy: 0.5745 - val_loss: 0.9573 - val_accuracy: 0.5718 - lr: 4.0000e-05\n",
      "Epoch 87/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9596 - accuracy: 0.5731\n",
      "Epoch 87: val_accuracy did not improve from 0.57344\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9596 - accuracy: 0.5731 - val_loss: 0.9571 - val_accuracy: 0.5731 - lr: 4.0000e-05\n",
      "Epoch 88/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9599 - accuracy: 0.5739\n",
      "Epoch 88: val_accuracy improved from 0.57344 to 0.57347, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9598 - accuracy: 0.5739 - val_loss: 0.9573 - val_accuracy: 0.5735 - lr: 4.0000e-05\n",
      "Epoch 89/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.5726\n",
      "Epoch 89: val_accuracy improved from 0.57347 to 0.57350, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9606 - accuracy: 0.5725 - val_loss: 0.9571 - val_accuracy: 0.5735 - lr: 4.0000e-05\n",
      "Epoch 90/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9612 - accuracy: 0.5743\n",
      "Epoch 90: val_accuracy improved from 0.57350 to 0.57401, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9615 - accuracy: 0.5742 - val_loss: 0.9575 - val_accuracy: 0.5740 - lr: 4.0000e-05\n",
      "Epoch 91/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9593 - accuracy: 0.5728\n",
      "Epoch 91: val_accuracy improved from 0.57401 to 0.57410, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9591 - accuracy: 0.5728 - val_loss: 0.9571 - val_accuracy: 0.5741 - lr: 4.0000e-05\n",
      "Epoch 92/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9603 - accuracy: 0.5735\n",
      "Epoch 92: val_accuracy improved from 0.57410 to 0.57438, saving model to /home/creeg/projects/audible/notebooks/rating_predictor.h5\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9604 - accuracy: 0.5735 - val_loss: 0.9568 - val_accuracy: 0.5744 - lr: 4.0000e-05\n",
      "Epoch 93/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9611 - accuracy: 0.5747\n",
      "Epoch 93: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9612 - accuracy: 0.5744 - val_loss: 0.9566 - val_accuracy: 0.5736 - lr: 4.0000e-05\n",
      "Epoch 94/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9594 - accuracy: 0.5740\n",
      "Epoch 94: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9595 - accuracy: 0.5739 - val_loss: 0.9568 - val_accuracy: 0.5733 - lr: 4.0000e-05\n",
      "Epoch 95/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9586 - accuracy: 0.5736\n",
      "Epoch 95: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9585 - accuracy: 0.5736 - val_loss: 0.9566 - val_accuracy: 0.5727 - lr: 4.0000e-05\n",
      "Epoch 96/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9592 - accuracy: 0.5731\n",
      "Epoch 96: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9592 - accuracy: 0.5731 - val_loss: 0.9567 - val_accuracy: 0.5715 - lr: 4.0000e-05\n",
      "Epoch 97/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9598 - accuracy: 0.5733\n",
      "Epoch 97: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9598 - accuracy: 0.5733 - val_loss: 0.9566 - val_accuracy: 0.5728 - lr: 4.0000e-05\n",
      "Epoch 98/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9586 - accuracy: 0.5744\n",
      "Epoch 98: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9588 - accuracy: 0.5743 - val_loss: 0.9563 - val_accuracy: 0.5732 - lr: 4.0000e-05\n",
      "Epoch 99/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9589 - accuracy: 0.5744\n",
      "Epoch 99: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9589 - accuracy: 0.5744 - val_loss: 0.9565 - val_accuracy: 0.5736 - lr: 4.0000e-05\n",
      "Epoch 100/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9598 - accuracy: 0.5753\n",
      "Epoch 100: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9598 - accuracy: 0.5754 - val_loss: 0.9563 - val_accuracy: 0.5727 - lr: 4.0000e-05\n",
      "Epoch 101/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9588 - accuracy: 0.5746\n",
      "Epoch 101: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9588 - accuracy: 0.5746 - val_loss: 0.9568 - val_accuracy: 0.5721 - lr: 4.0000e-05\n",
      "Epoch 102/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9588 - accuracy: 0.5745\n",
      "Epoch 102: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9588 - accuracy: 0.5745 - val_loss: 0.9565 - val_accuracy: 0.5728 - lr: 4.0000e-05\n",
      "Epoch 103/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9590 - accuracy: 0.5715\n",
      "Epoch 103: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9590 - accuracy: 0.5715 - val_loss: 0.9566 - val_accuracy: 0.5726 - lr: 4.0000e-05\n",
      "Epoch 104/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9583 - accuracy: 0.5736\n",
      "Epoch 104: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9582 - accuracy: 0.5737 - val_loss: 0.9566 - val_accuracy: 0.5740 - lr: 4.0000e-05\n",
      "Epoch 105/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9595 - accuracy: 0.5741\n",
      "Epoch 105: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9596 - accuracy: 0.5740 - val_loss: 0.9569 - val_accuracy: 0.5738 - lr: 4.0000e-05\n",
      "Epoch 106/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9603 - accuracy: 0.5733\n",
      "Epoch 106: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9602 - accuracy: 0.5733 - val_loss: 0.9565 - val_accuracy: 0.5725 - lr: 4.0000e-05\n",
      "Epoch 107/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9581 - accuracy: 0.5753\n",
      "Epoch 107: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9578 - accuracy: 0.5754 - val_loss: 0.9564 - val_accuracy: 0.5728 - lr: 4.0000e-05\n",
      "Epoch 108/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9597 - accuracy: 0.5734\n",
      "Epoch 108: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9596 - accuracy: 0.5734 - val_loss: 0.9564 - val_accuracy: 0.5731 - lr: 4.0000e-05\n",
      "Epoch 109/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9583 - accuracy: 0.5745\n",
      "Epoch 109: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 17s 4ms/step - loss: 0.9583 - accuracy: 0.5745 - val_loss: 0.9567 - val_accuracy: 0.5722 - lr: 8.0000e-06\n",
      "Epoch 110/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5770\n",
      "Epoch 110: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9570 - accuracy: 0.5770 - val_loss: 0.9564 - val_accuracy: 0.5726 - lr: 8.0000e-06\n",
      "Epoch 111/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9579 - accuracy: 0.5749\n",
      "Epoch 111: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9580 - accuracy: 0.5749 - val_loss: 0.9562 - val_accuracy: 0.5725 - lr: 8.0000e-06\n",
      "Epoch 112/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9583 - accuracy: 0.5757\n",
      "Epoch 112: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9582 - accuracy: 0.5757 - val_loss: 0.9560 - val_accuracy: 0.5726 - lr: 8.0000e-06\n",
      "Epoch 113/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9586 - accuracy: 0.5731\n",
      "Epoch 113: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9586 - accuracy: 0.5731 - val_loss: 0.9562 - val_accuracy: 0.5725 - lr: 8.0000e-06\n",
      "Epoch 114/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.5755\n",
      "Epoch 114: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9576 - accuracy: 0.5755 - val_loss: 0.9563 - val_accuracy: 0.5732 - lr: 8.0000e-06\n",
      "Epoch 115/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.5749\n",
      "Epoch 115: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9575 - accuracy: 0.5749 - val_loss: 0.9560 - val_accuracy: 0.5727 - lr: 8.0000e-06\n",
      "Epoch 116/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9584 - accuracy: 0.5748\n",
      "Epoch 116: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9585 - accuracy: 0.5747 - val_loss: 0.9561 - val_accuracy: 0.5731 - lr: 8.0000e-06\n",
      "Epoch 117/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9559 - accuracy: 0.5750\n",
      "Epoch 117: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9560 - accuracy: 0.5750 - val_loss: 0.9564 - val_accuracy: 0.5738 - lr: 8.0000e-06\n",
      "Epoch 118/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5765\n",
      "Epoch 118: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9567 - accuracy: 0.5766 - val_loss: 0.9561 - val_accuracy: 0.5733 - lr: 8.0000e-06\n",
      "Epoch 119/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9586 - accuracy: 0.5743\n",
      "Epoch 119: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9586 - accuracy: 0.5742 - val_loss: 0.9564 - val_accuracy: 0.5729 - lr: 8.0000e-06\n",
      "Epoch 120/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9578 - accuracy: 0.5746\n",
      "Epoch 120: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9578 - accuracy: 0.5746 - val_loss: 0.9560 - val_accuracy: 0.5731 - lr: 8.0000e-06\n",
      "Epoch 121/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5751\n",
      "Epoch 121: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9570 - accuracy: 0.5751 - val_loss: 0.9561 - val_accuracy: 0.5729 - lr: 8.0000e-06\n",
      "Epoch 122/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 0.9551 - accuracy: 0.5745\n",
      "Epoch 122: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9552 - accuracy: 0.5744 - val_loss: 0.9560 - val_accuracy: 0.5727 - lr: 8.0000e-06\n",
      "Epoch 123/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9582 - accuracy: 0.5743\n",
      "Epoch 123: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9582 - accuracy: 0.5741 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 1.6000e-06\n",
      "Epoch 124/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.5743\n",
      "Epoch 124: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9566 - accuracy: 0.5744 - val_loss: 0.9559 - val_accuracy: 0.5731 - lr: 1.6000e-06\n",
      "Epoch 125/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5742\n",
      "Epoch 125: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9572 - accuracy: 0.5743 - val_loss: 0.9562 - val_accuracy: 0.5733 - lr: 1.6000e-06\n",
      "Epoch 126/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9583 - accuracy: 0.5747\n",
      "Epoch 126: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9581 - accuracy: 0.5748 - val_loss: 0.9561 - val_accuracy: 0.5732 - lr: 1.6000e-06\n",
      "Epoch 127/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9583 - accuracy: 0.5745\n",
      "Epoch 127: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9582 - accuracy: 0.5745 - val_loss: 0.9561 - val_accuracy: 0.5727 - lr: 1.6000e-06\n",
      "Epoch 128/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9568 - accuracy: 0.5745\n",
      "Epoch 128: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9569 - accuracy: 0.5746 - val_loss: 0.9562 - val_accuracy: 0.5730 - lr: 1.6000e-06\n",
      "Epoch 129/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9561 - accuracy: 0.5757\n",
      "Epoch 129: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9561 - accuracy: 0.5757 - val_loss: 0.9562 - val_accuracy: 0.5733 - lr: 1.6000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9577 - accuracy: 0.5756\n",
      "Epoch 130: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9578 - accuracy: 0.5755 - val_loss: 0.9561 - val_accuracy: 0.5734 - lr: 1.6000e-06\n",
      "Epoch 131/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9581 - accuracy: 0.5745\n",
      "Epoch 131: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9581 - accuracy: 0.5745 - val_loss: 0.9563 - val_accuracy: 0.5732 - lr: 1.6000e-06\n",
      "Epoch 132/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5750\n",
      "Epoch 132: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9570 - accuracy: 0.5750 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 1.6000e-06\n",
      "Epoch 133/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9579 - accuracy: 0.5744\n",
      "Epoch 133: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9579 - accuracy: 0.5744 - val_loss: 0.9562 - val_accuracy: 0.5726 - lr: 3.2000e-07\n",
      "Epoch 134/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.5745\n",
      "Epoch 134: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9568 - accuracy: 0.5744 - val_loss: 0.9558 - val_accuracy: 0.5732 - lr: 3.2000e-07\n",
      "Epoch 135/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9563 - accuracy: 0.5746\n",
      "Epoch 135: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9564 - accuracy: 0.5745 - val_loss: 0.9562 - val_accuracy: 0.5730 - lr: 3.2000e-07\n",
      "Epoch 136/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9564 - accuracy: 0.5758\n",
      "Epoch 136: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9564 - accuracy: 0.5758 - val_loss: 0.9560 - val_accuracy: 0.5735 - lr: 3.2000e-07\n",
      "Epoch 137/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9572 - accuracy: 0.5742\n",
      "Epoch 137: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9571 - accuracy: 0.5743 - val_loss: 0.9564 - val_accuracy: 0.5730 - lr: 3.2000e-07\n",
      "Epoch 138/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9567 - accuracy: 0.5753\n",
      "Epoch 138: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9567 - accuracy: 0.5753 - val_loss: 0.9562 - val_accuracy: 0.5730 - lr: 3.2000e-07\n",
      "Epoch 139/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.5760\n",
      "Epoch 139: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9573 - accuracy: 0.5760 - val_loss: 0.9564 - val_accuracy: 0.5732 - lr: 3.2000e-07\n",
      "Epoch 140/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9555 - accuracy: 0.5761\n",
      "Epoch 140: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9556 - accuracy: 0.5761 - val_loss: 0.9560 - val_accuracy: 0.5726 - lr: 3.2000e-07\n",
      "Epoch 141/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9584 - accuracy: 0.5742\n",
      "Epoch 141: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9585 - accuracy: 0.5742 - val_loss: 0.9561 - val_accuracy: 0.5729 - lr: 3.2000e-07\n",
      "Epoch 142/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.5733\n",
      "Epoch 142: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9573 - accuracy: 0.5736 - val_loss: 0.9565 - val_accuracy: 0.5724 - lr: 3.2000e-07\n",
      "Epoch 143/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.5752\n",
      "Epoch 143: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9577 - accuracy: 0.5753 - val_loss: 0.9560 - val_accuracy: 0.5733 - lr: 3.2000e-07\n",
      "Epoch 144/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.5750\n",
      "Epoch 144: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9578 - accuracy: 0.5750 - val_loss: 0.9561 - val_accuracy: 0.5736 - lr: 3.2000e-07\n",
      "Epoch 145/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.5750\n",
      "Epoch 145: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9577 - accuracy: 0.5750 - val_loss: 0.9560 - val_accuracy: 0.5728 - lr: 6.4000e-08\n",
      "Epoch 146/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.5748\n",
      "Epoch 146: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 13s 3ms/step - loss: 0.9569 - accuracy: 0.5748 - val_loss: 0.9564 - val_accuracy: 0.5728 - lr: 6.4000e-08\n",
      "Epoch 147/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9558 - accuracy: 0.5748\n",
      "Epoch 147: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9560 - accuracy: 0.5747 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 6.4000e-08\n",
      "Epoch 148/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9576 - accuracy: 0.5751\n",
      "Epoch 148: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9576 - accuracy: 0.5751 - val_loss: 0.9561 - val_accuracy: 0.5732 - lr: 6.4000e-08\n",
      "Epoch 149/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.5755\n",
      "Epoch 149: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9564 - accuracy: 0.5756 - val_loss: 0.9561 - val_accuracy: 0.5736 - lr: 6.4000e-08\n",
      "Epoch 150/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5740\n",
      "Epoch 150: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9574 - accuracy: 0.5741 - val_loss: 0.9561 - val_accuracy: 0.5735 - lr: 6.4000e-08\n",
      "Epoch 151/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9574 - accuracy: 0.5749\n",
      "Epoch 151: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9575 - accuracy: 0.5745 - val_loss: 0.9564 - val_accuracy: 0.5728 - lr: 6.4000e-08\n",
      "Epoch 152/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5756\n",
      "Epoch 152: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9571 - accuracy: 0.5757 - val_loss: 0.9559 - val_accuracy: 0.5734 - lr: 6.4000e-08\n",
      "Epoch 153/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.5752\n",
      "Epoch 153: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9577 - accuracy: 0.5752 - val_loss: 0.9563 - val_accuracy: 0.5730 - lr: 6.4000e-08\n",
      "Epoch 154/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5750\n",
      "Epoch 154: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9572 - accuracy: 0.5749 - val_loss: 0.9561 - val_accuracy: 0.5731 - lr: 6.4000e-08\n",
      "Epoch 155/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9568 - accuracy: 0.5748\n",
      "Epoch 155: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9566 - accuracy: 0.5749 - val_loss: 0.9564 - val_accuracy: 0.5727 - lr: 1.2800e-08\n",
      "Epoch 156/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9563 - accuracy: 0.5751\n",
      "Epoch 156: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9564 - accuracy: 0.5752 - val_loss: 0.9563 - val_accuracy: 0.5733 - lr: 1.2800e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9581 - accuracy: 0.5755\n",
      "Epoch 157: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9581 - accuracy: 0.5755 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 1.2800e-08\n",
      "Epoch 158/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5740\n",
      "Epoch 158: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9567 - accuracy: 0.5742 - val_loss: 0.9561 - val_accuracy: 0.5730 - lr: 1.2800e-08\n",
      "Epoch 159/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.5756\n",
      "Epoch 159: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9565 - accuracy: 0.5758 - val_loss: 0.9562 - val_accuracy: 0.5730 - lr: 1.2800e-08\n",
      "Epoch 160/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9582 - accuracy: 0.5730\n",
      "Epoch 160: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9582 - accuracy: 0.5730 - val_loss: 0.9562 - val_accuracy: 0.5733 - lr: 1.2800e-08\n",
      "Epoch 161/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.5772\n",
      "Epoch 161: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9566 - accuracy: 0.5772 - val_loss: 0.9559 - val_accuracy: 0.5733 - lr: 1.2800e-08\n",
      "Epoch 162/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9574 - accuracy: 0.5746\n",
      "Epoch 162: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9578 - accuracy: 0.5744 - val_loss: 0.9561 - val_accuracy: 0.5737 - lr: 1.2800e-08\n",
      "Epoch 163/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.5763\n",
      "Epoch 163: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9565 - accuracy: 0.5761 - val_loss: 0.9561 - val_accuracy: 0.5729 - lr: 1.2800e-08\n",
      "Epoch 164/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9577 - accuracy: 0.5746\n",
      "Epoch 164: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9578 - accuracy: 0.5746 - val_loss: 0.9562 - val_accuracy: 0.5735 - lr: 1.2800e-08\n",
      "Epoch 165/200\n",
      "3840/3856 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5736\n",
      "Epoch 165: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9571 - accuracy: 0.5735 - val_loss: 0.9564 - val_accuracy: 0.5731 - lr: 2.5600e-09\n",
      "Epoch 166/200\n",
      "3844/3856 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.5764\n",
      "Epoch 166: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9575 - accuracy: 0.5764 - val_loss: 0.9561 - val_accuracy: 0.5733 - lr: 2.5600e-09\n",
      "Epoch 167/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9576 - accuracy: 0.5764\n",
      "Epoch 167: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9576 - accuracy: 0.5763 - val_loss: 0.9558 - val_accuracy: 0.5735 - lr: 2.5600e-09\n",
      "Epoch 168/200\n",
      "3852/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5745\n",
      "Epoch 168: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9575 - accuracy: 0.5745 - val_loss: 0.9561 - val_accuracy: 0.5728 - lr: 2.5600e-09\n",
      "Epoch 169/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9574 - accuracy: 0.5731\n",
      "Epoch 169: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9575 - accuracy: 0.5732 - val_loss: 0.9561 - val_accuracy: 0.5731 - lr: 2.5600e-09\n",
      "Epoch 170/200\n",
      "3849/3856 [============================>.] - ETA: 0s - loss: 0.9564 - accuracy: 0.5740\n",
      "Epoch 170: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9565 - accuracy: 0.5739 - val_loss: 0.9561 - val_accuracy: 0.5736 - lr: 2.5600e-09\n",
      "Epoch 171/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9562 - accuracy: 0.5747\n",
      "Epoch 171: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9563 - accuracy: 0.5747 - val_loss: 0.9563 - val_accuracy: 0.5727 - lr: 2.5600e-09\n",
      "Epoch 172/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9564 - accuracy: 0.5760\n",
      "Epoch 172: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9564 - accuracy: 0.5760 - val_loss: 0.9561 - val_accuracy: 0.5727 - lr: 2.5600e-09\n",
      "Epoch 173/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9568 - accuracy: 0.5758\n",
      "Epoch 173: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9569 - accuracy: 0.5757 - val_loss: 0.9560 - val_accuracy: 0.5734 - lr: 2.5600e-09\n",
      "Epoch 174/200\n",
      "3853/3856 [============================>.] - ETA: 0s - loss: 0.9567 - accuracy: 0.5766\n",
      "Epoch 174: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9567 - accuracy: 0.5767 - val_loss: 0.9559 - val_accuracy: 0.5732 - lr: 2.5600e-09\n",
      "Epoch 175/200\n",
      "3847/3856 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.5751\n",
      "Epoch 175: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9577 - accuracy: 0.5750 - val_loss: 0.9561 - val_accuracy: 0.5731 - lr: 5.1200e-10\n",
      "Epoch 176/200\n",
      "3854/3856 [============================>.] - ETA: 0s - loss: 0.9584 - accuracy: 0.5749\n",
      "Epoch 176: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9583 - accuracy: 0.5750 - val_loss: 0.9560 - val_accuracy: 0.5734 - lr: 5.1200e-10\n",
      "Epoch 177/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9562 - accuracy: 0.5755\n",
      "Epoch 177: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9562 - accuracy: 0.5755 - val_loss: 0.9559 - val_accuracy: 0.5738 - lr: 5.1200e-10\n",
      "Epoch 178/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9570 - accuracy: 0.5747\n",
      "Epoch 178: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9570 - accuracy: 0.5746 - val_loss: 0.9561 - val_accuracy: 0.5732 - lr: 5.1200e-10\n",
      "Epoch 179/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.5737\n",
      "Epoch 179: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9567 - accuracy: 0.5737 - val_loss: 0.9561 - val_accuracy: 0.5734 - lr: 5.1200e-10\n",
      "Epoch 180/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9568 - accuracy: 0.5755\n",
      "Epoch 180: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9568 - accuracy: 0.5755 - val_loss: 0.9559 - val_accuracy: 0.5737 - lr: 5.1200e-10\n",
      "Epoch 181/200\n",
      "3851/3856 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.5753\n",
      "Epoch 181: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9564 - accuracy: 0.5754 - val_loss: 0.9562 - val_accuracy: 0.5732 - lr: 5.1200e-10\n",
      "Epoch 182/200\n",
      "3843/3856 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.5748\n",
      "Epoch 182: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9577 - accuracy: 0.5748 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 5.1200e-10\n",
      "Epoch 183/200\n",
      "3841/3856 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5736\n",
      "Epoch 183: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9567 - accuracy: 0.5737 - val_loss: 0.9563 - val_accuracy: 0.5733 - lr: 5.1200e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9569 - accuracy: 0.5759\n",
      "Epoch 184: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9569 - accuracy: 0.5759 - val_loss: 0.9561 - val_accuracy: 0.5730 - lr: 5.1200e-10\n",
      "Epoch 185/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9565 - accuracy: 0.5752\n",
      "Epoch 185: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9568 - accuracy: 0.5751 - val_loss: 0.9560 - val_accuracy: 0.5737 - lr: 1.0240e-10\n",
      "Epoch 186/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9569 - accuracy: 0.5753\n",
      "Epoch 186: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9569 - accuracy: 0.5753 - val_loss: 0.9560 - val_accuracy: 0.5730 - lr: 1.0240e-10\n",
      "Epoch 187/200\n",
      "3856/3856 [==============================] - ETA: 0s - loss: 0.9575 - accuracy: 0.5758\n",
      "Epoch 187: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9575 - accuracy: 0.5758 - val_loss: 0.9561 - val_accuracy: 0.5732 - lr: 1.0240e-10\n",
      "Epoch 188/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9560 - accuracy: 0.5755\n",
      "Epoch 188: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 16s 4ms/step - loss: 0.9561 - accuracy: 0.5754 - val_loss: 0.9558 - val_accuracy: 0.5739 - lr: 1.0240e-10\n",
      "Epoch 189/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.5745\n",
      "Epoch 189: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9576 - accuracy: 0.5744 - val_loss: 0.9560 - val_accuracy: 0.5735 - lr: 1.0240e-10\n",
      "Epoch 190/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5739\n",
      "Epoch 190: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9572 - accuracy: 0.5739 - val_loss: 0.9560 - val_accuracy: 0.5731 - lr: 1.0240e-10\n",
      "Epoch 191/200\n",
      "3846/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5757\n",
      "Epoch 191: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9573 - accuracy: 0.5758 - val_loss: 0.9562 - val_accuracy: 0.5733 - lr: 1.0240e-10\n",
      "Epoch 192/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9563 - accuracy: 0.5742\n",
      "Epoch 192: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9563 - accuracy: 0.5742 - val_loss: 0.9559 - val_accuracy: 0.5733 - lr: 1.0240e-10\n",
      "Epoch 193/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5762\n",
      "Epoch 193: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9569 - accuracy: 0.5762 - val_loss: 0.9561 - val_accuracy: 0.5728 - lr: 1.0240e-10\n",
      "Epoch 194/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9573 - accuracy: 0.5754\n",
      "Epoch 194: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9574 - accuracy: 0.5754 - val_loss: 0.9563 - val_accuracy: 0.5731 - lr: 1.0240e-10\n",
      "Epoch 195/200\n",
      "3855/3856 [============================>.] - ETA: 0s - loss: 0.9579 - accuracy: 0.5750\n",
      "Epoch 195: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9580 - accuracy: 0.5750 - val_loss: 0.9560 - val_accuracy: 0.5735 - lr: 2.0480e-11\n",
      "Epoch 196/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9571 - accuracy: 0.5749\n",
      "Epoch 196: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9570 - accuracy: 0.5749 - val_loss: 0.9562 - val_accuracy: 0.5731 - lr: 2.0480e-11\n",
      "Epoch 197/200\n",
      "3845/3856 [============================>.] - ETA: 0s - loss: 0.9558 - accuracy: 0.5764\n",
      "Epoch 197: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9558 - accuracy: 0.5766 - val_loss: 0.9559 - val_accuracy: 0.5730 - lr: 2.0480e-11\n",
      "Epoch 198/200\n",
      "3842/3856 [============================>.] - ETA: 0s - loss: 0.9580 - accuracy: 0.5761\n",
      "Epoch 198: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9579 - accuracy: 0.5761 - val_loss: 0.9562 - val_accuracy: 0.5734 - lr: 2.0480e-11\n",
      "Epoch 199/200\n",
      "3848/3856 [============================>.] - ETA: 0s - loss: 0.9574 - accuracy: 0.5764\n",
      "Epoch 199: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 15s 4ms/step - loss: 0.9573 - accuracy: 0.5766 - val_loss: 0.9560 - val_accuracy: 0.5728 - lr: 2.0480e-11\n",
      "Epoch 200/200\n",
      "3850/3856 [============================>.] - ETA: 0s - loss: 0.9575 - accuracy: 0.5752\n",
      "Epoch 200: val_accuracy did not improve from 0.57438\n",
      "3856/3856 [==============================] - 14s 4ms/step - loss: 0.9575 - accuracy: 0.5753 - val_loss: 0.9560 - val_accuracy: 0.5729 - lr: 2.0480e-11\n"
     ]
    }
   ],
   "source": [
    "his = rating_predictor.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.3, callbacks=cb)\n",
    "rating_predictor.save('rating_predictor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9719ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861/861 [==============================] - 1s 1ms/step - loss: 0.9631 - accuracy: 0.5735\n",
      " the model accuracy is 0.5734567642211914 \n"
     ]
    }
   ],
   "source": [
    "model_saved = keras.models.load_model('rating_predictor.h5')\n",
    "score = model_saved.evaluate(X_test, y_test)\n",
    "print(f\" the model accuracy is {score[1]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9b94d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['accuracy'])\n",
    "plt.plot(his.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b3d62d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d580c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(n_jobs=-1, random_state=42, oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea3ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c046dd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(n_jobs=-1, oob_score=True,\n",
       "                                              random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [4, 5, 6, 7, 8],\n",
       "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(n_jobs=-1, oob_score=True,\n",
       "                                              random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [4, 5, 6, 7, 8],\n",
       "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [200, 500]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(n_jobs=-1, oob_score=True,\n",
       "                                              random_state=42),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [4, 5, 6, 7, 8],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 500]})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf22f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 8,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec3e4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1=RandomForestClassifier(n_jobs=-1\n",
    "                            , random_state=42\n",
    "                            , max_features='auto'\n",
    "                            , n_estimators= 500\n",
    "                            , max_depth=8\n",
    "                            , criterion='gini'\n",
    "                            , oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b1f78b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=8, max_features=&#x27;auto&#x27;, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=8, max_features=&#x27;auto&#x27;, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features='auto', n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb82b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_pred = rfc1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd709037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest on CV data:  0.2739651416122004\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,rcf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8371c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
